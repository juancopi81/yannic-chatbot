{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXOAtWUp2j1c8tno7GfWYM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juancopi81/yannic-chatbot/blob/main/Chatbot_Ada_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qO-xFYyKCSOP",
        "outputId": "647c3670-aa80-4368-98ef-3fccf1ac52a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.8.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.26.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting requests>=2.19.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile>=2\n",
            "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (1.26.14)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: requests, pycryptodomex, blobfile, tiktoken\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed blobfile-2.0.1 pycryptodomex-3.16.0 requests-2.28.2 tiktoken-0.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install datasets openai transformers tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import openai\n",
        "import tiktoken\n",
        "from transformers import GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "jNZWBDg7Dg3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
        "MAX_SECTION_LEN = 2000\n",
        "COMPLETIONS_API_PARAMS = {\n",
        "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 500,\n",
        "    \"model\": COMPLETIONS_MODEL,\n",
        "}\n",
        "\n",
        "hf_ds = \"juancopi81/yannic_ada_embeddings\"\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "HEADER = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"This is not covered in my videos.\" Try imitating the style of the provided context. \\n\\nContext:\\n\"\"\"\n",
        "RESPONSE_SOURCES = \" For more information, check out my following videos: \""
      ],
      "metadata": {
        "id": "6YcxHf1FI-wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query separator to help the model distinguish between separate pieces of text.\n",
        "SEPARATOR = \"\\n* \"\n",
        "ENCODING = \"cl100k_base\"  # encoding for text-embedding-ada-002\n",
        "\n",
        "encoding = tiktoken.get_encoding(ENCODING)\n",
        "separator_len = len(encoding.encode(SEPARATOR))\n",
        "\n",
        "f\"Context separator contains {separator_len} tokens\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1txDjptPUgmn",
        "outputId": "7693c391-1ca2-409c-b725-360d69a18779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context separator contains 3 tokens'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "zVpdCuQlOqkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text: str) -> int:\n",
        "    \"\"\"count the number of tokens in a string\"\"\"\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "def load_embeddings(hf_ds: str) -> dict:\n",
        "    \"\"\"\n",
        "    Read the document embeddings and their keys from a HuggingFace dataset.\n",
        "    \n",
        "    hf_ds is the name of the HF dataset with exactly these named columns: \n",
        "        \"TITLE\", \"URL\", \"TRANSCRIPTION\", \"transcription_length\", \"text\", \"ada_embedding\"\n",
        "    \"\"\"\n",
        "    hf_ds = load_dataset(hf_ds, split=\"train\")\n",
        "    hf_ds.set_format(\"pandas\")\n",
        "    df = hf_ds[:]\n",
        "    df.ada_embedding = df.ada_embedding.apply(literal_eval)\n",
        "    df[\"idx\"] = df.index\n",
        "    return {\n",
        "        (r.idx, r.TITLE, r.URL): r.ada_embedding for idx, r in df.iterrows()\n",
        "    }\n",
        "\n",
        "def create_dataframe(hf_ds: str):\n",
        "    hf_ds = load_dataset(hf_ds, split=\"train\")\n",
        "    hf_ds.set_format(\"pandas\")\n",
        "    df = hf_ds[:]\n",
        "    df[\"num_tokens\"] = df[\"text\"].map(count_tokens)\n",
        "    df[\"idx\"] = df.index\n",
        "    df = df.set_index([\"idx\", \"TITLE\", \"URL\"])\n",
        "    return df\n",
        "\n",
        "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list:\n",
        "    result = openai.Embedding.create(\n",
        "      model=model,\n",
        "      input=text\n",
        "    )\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "def vector_similarity(x: list, y: list) -> float:\n",
        "    \"\"\"\n",
        "    Returns the similarity between two vectors.\n",
        "    \n",
        "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
        "    \"\"\"\n",
        "    return np.dot(np.array(x), np.array(y))\n",
        "\n",
        "def order_document_sections_by_query_similarity(query: str, contexts: dict) -> list:\n",
        "    \"\"\"\n",
        "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
        "    to find the most relevant sections. \n",
        "    \n",
        "    Return the list of document sections, sorted by relevance in descending order.\n",
        "    \"\"\"\n",
        "    query_embedding = get_embedding(query)\n",
        "    \n",
        "    document_similarities = sorted([\n",
        "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
        "    ], reverse=True)\n",
        "    \n",
        "    return document_similarities\n",
        "\n",
        "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> tuple:\n",
        "    \"\"\"\n",
        "    Fetch relevant \n",
        "    \"\"\"\n",
        "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
        "    \n",
        "    chosen_sections = []\n",
        "    chosen_sections_len = 0\n",
        "    chosen_sections_indexes = []\n",
        "     \n",
        "    for _, section_index in most_relevant_document_sections:\n",
        "        # Add contexts until we run out of space.        \n",
        "        document_section = df.loc[section_index]\n",
        "        \n",
        "        chosen_sections_len += document_section.num_tokens + separator_len\n",
        "        if chosen_sections_len > MAX_SECTION_LEN:\n",
        "            break\n",
        "            \n",
        "        chosen_sections.append(SEPARATOR + document_section.text.replace(\"\\n\", \" \"))\n",
        "        chosen_sections_indexes.append(str(section_index))\n",
        "            \n",
        "    # Useful diagnostic information\n",
        "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
        "    print(\"\\n\".join(chosen_sections_indexes))\n",
        "    \n",
        "    header = HEADER\n",
        "    \n",
        "    return (header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\",\n",
        "            chosen_sections_indexes)\n",
        "\n",
        "def answer_query_with_context(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    document_embeddings: dict,\n",
        "    show_prompt: bool = False\n",
        ") -> str:\n",
        "    prompt, sources = construct_prompt(\n",
        "        query,\n",
        "        document_embeddings,\n",
        "        df\n",
        "    )\n",
        "    \n",
        "    if show_prompt:\n",
        "        print(prompt)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "                prompt=prompt,\n",
        "                **COMPLETIONS_API_PARAMS\n",
        "            )\n",
        "    \n",
        "    res_sources = RESPONSE_SOURCES\n",
        "    for source in sources[:2]:\n",
        "        src_lst = eval(source)\n",
        "        title = \"\".join(src_lst[1])\n",
        "        url = \"\".join(src_lst[2])\n",
        "        final_src = title + \" \" + url\n",
        "        res_sources += \" \" + final_src\n",
        "\n",
        "    return response[\"choices\"][0][\"text\"].strip(\" \\n\") + res_sources"
      ],
      "metadata": {
        "id": "1M_qTpqpFOLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = create_dataframe(hf_ds)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "YmUPtmuLR9Dr",
        "outputId": "90c8e232-20a6-404c-a0c1-e0c928d3c8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration juancopi81--yannic_ada_embeddings-a80e7544f06dfd9d\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/juancopi81___parquet/juancopi81--yannic_ada_embeddings-a80e7544f06dfd9d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                        TRANSCRIPTION  \\\n",
              "idx TITLE                                              URL                                                                                              \n",
              "0   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  Hello there, today we'll look at Glide towards...   \n",
              "1   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  I'm going to paint this area right here. And I...   \n",
              "2   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  model substantially to make very misleading pi...   \n",
              "3   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  the cat, and predict and learn to predict the ...   \n",
              "4   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  at each step is small enough, the posterior is...   \n",
              "\n",
              "                                                                                                    transcription_length  \\\n",
              "idx TITLE                                              URL                                                                 \n",
              "0   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84                   500   \n",
              "1   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84                   500   \n",
              "2   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84                   500   \n",
              "3   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84                   500   \n",
              "4   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84                   500   \n",
              "\n",
              "                                                                                                                                                 text  \\\n",
              "idx TITLE                                              URL                                                                                              \n",
              "0   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  GLIDE: Towards Photorealistic Image Generation...   \n",
              "1   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  GLIDE: Towards Photorealistic Image Generation...   \n",
              "2   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  GLIDE: Towards Photorealistic Image Generation...   \n",
              "3   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  GLIDE: Towards Photorealistic Image Generation...   \n",
              "4   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  GLIDE: Towards Photorealistic Image Generation...   \n",
              "\n",
              "                                                                                                                                        ada_embedding  \\\n",
              "idx TITLE                                              URL                                                                                              \n",
              "0   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  [-0.023338761180639267, 0.01196559239178896, -...   \n",
              "1   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  [-0.02665344625711441, 0.006354713812470436, 0...   \n",
              "2   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  [-0.023775776848196983, -0.0021778957452625036...   \n",
              "3   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  [-0.01872330904006958, 0.016276435926556587, -...   \n",
              "4   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84  [-0.016091231256723404, 0.017658470198512077, ...   \n",
              "\n",
              "                                                                                                    num_tokens  \n",
              "idx TITLE                                              URL                                                      \n",
              "0   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84         644  \n",
              "1   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84         625  \n",
              "2   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84         625  \n",
              "3   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84         621  \n",
              "4   GLIDE: Towards Photorealistic Image Generation ... https://www.youtube.com/watch?v=gwI6g1pBD84         621  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d27dea78-2622-4ad7-af3d-613f93fa65dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>TRANSCRIPTION</th>\n",
              "      <th>transcription_length</th>\n",
              "      <th>text</th>\n",
              "      <th>ada_embedding</th>\n",
              "      <th>num_tokens</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>URL</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</th>\n",
              "      <th>https://www.youtube.com/watch?v=gwI6g1pBD84</th>\n",
              "      <td>Hello there, today we'll look at Glide towards...</td>\n",
              "      <td>500</td>\n",
              "      <td>GLIDE: Towards Photorealistic Image Generation...</td>\n",
              "      <td>[-0.023338761180639267, 0.01196559239178896, -...</td>\n",
              "      <td>644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</th>\n",
              "      <th>https://www.youtube.com/watch?v=gwI6g1pBD84</th>\n",
              "      <td>I'm going to paint this area right here. And I...</td>\n",
              "      <td>500</td>\n",
              "      <td>GLIDE: Towards Photorealistic Image Generation...</td>\n",
              "      <td>[-0.02665344625711441, 0.006354713812470436, 0...</td>\n",
              "      <td>625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <th>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</th>\n",
              "      <th>https://www.youtube.com/watch?v=gwI6g1pBD84</th>\n",
              "      <td>model substantially to make very misleading pi...</td>\n",
              "      <td>500</td>\n",
              "      <td>GLIDE: Towards Photorealistic Image Generation...</td>\n",
              "      <td>[-0.023775776848196983, -0.0021778957452625036...</td>\n",
              "      <td>625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <th>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</th>\n",
              "      <th>https://www.youtube.com/watch?v=gwI6g1pBD84</th>\n",
              "      <td>the cat, and predict and learn to predict the ...</td>\n",
              "      <td>500</td>\n",
              "      <td>GLIDE: Towards Photorealistic Image Generation...</td>\n",
              "      <td>[-0.01872330904006958, 0.016276435926556587, -...</td>\n",
              "      <td>621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <th>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</th>\n",
              "      <th>https://www.youtube.com/watch?v=gwI6g1pBD84</th>\n",
              "      <td>at each step is small enough, the posterior is...</td>\n",
              "      <td>500</td>\n",
              "      <td>GLIDE: Towards Photorealistic Image Generation...</td>\n",
              "      <td>[-0.016091231256723404, 0.017658470198512077, ...</td>\n",
              "      <td>621</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d27dea78-2622-4ad7-af3d-613f93fa65dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d27dea78-2622-4ad7-af3d-613f93fa65dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d27dea78-2622-4ad7-af3d-613f93fa65dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_embeddings = load_embeddings(\"juancopi81/yannic_ada_embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuzAaXqlFoEV",
        "outputId": "943c33fb-c6f1-4b07-aa15-a291fefbd6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration juancopi81--yannic_ada_embeddings-a80e7544f06dfd9d\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/juancopi81___parquet/juancopi81--yannic_ada_embeddings-a80e7544f06dfd9d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example embedding:\n",
        "example_entry = list(document_embeddings.items())[0]\n",
        "print(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEGgtFFjGicX",
        "outputId": "246977ef-3531-44c2-f656-e26eb54763ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models', 'https://www.youtube.com/watch?v=gwI6g1pBD84') : [-0.023338761180639267, 0.01196559239178896, -0.00887258630245924, 0.011793375946581364, 0.016050564125180244]... (1536 entries)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "order_document_sections_by_query_similarity(\"Is OpenAI 'open'?\", document_embeddings)[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVyL8f6WJV6A",
        "outputId": "d8b1f1ce-3edc-4ed8-d52b-bcc77eda1cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.8543818997558141,\n",
              "  (1624,\n",
              "   '[ML News] AI-generated patent approved | Germany gets an analog to OpenAI | ML cheats video games',\n",
              "   'https://www.youtube.com/watch?v=SPOqoI0zOPQ')),\n",
              " (0.8488043317477918,\n",
              "  (250,\n",
              "   '[ML News] BLOOM: 176B Open-Source | Chinese Brain-Scale Computer | Meta AI: No Language Left Behind',\n",
              "   'https://www.youtube.com/watch?v=W3mrgqtm5R4')),\n",
              " (0.8472144938090321,\n",
              "  (148,\n",
              "   \"[ML News] OpenAI's Whisper | Meta Reads Brain Waves | AI Wins Art Fair, Annoys Humans\",\n",
              "   'https://www.youtube.com/watch?v=S-7r0-oysaU'))]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt, sources = construct_prompt(\n",
        "    \"Is OpenAI 'open'?\",\n",
        "    document_embeddings,\n",
        "    df\n",
        ")\n",
        "\n",
        "print(\"===\\n\", prompt)"
      ],
      "metadata": {
        "id": "F5TEZXUcJvjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b4a0ea-c362-48a6-fc8e-097b367b984e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 3 document sections:\n",
            "(1624, '[ML News] AI-generated patent approved | Germany gets an analog to OpenAI | ML cheats video games', 'https://www.youtube.com/watch?v=SPOqoI0zOPQ')\n",
            "(250, '[ML News] BLOOM: 176B Open-Source | Chinese Brain-Scale Computer | Meta AI: No Language Left Behind', 'https://www.youtube.com/watch?v=W3mrgqtm5R4')\n",
            "(148, \"[ML News] OpenAI's Whisper | Meta Reads Brain Waves | AI Wins Art Fair, Annoys Humans\", 'https://www.youtube.com/watch?v=S-7r0-oysaU')\n",
            "===\n",
            " Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"This is not covered in my videos.\" Try imitating the style of the provided context. \n",
            "\n",
            "Context:\n",
            "\n",
            "* [ML News] AI-generated patent approved | Germany gets an analog to OpenAI | ML cheats video games: which is 27 million in real money in a series a founding co led by early bird VC, Lake star and UBC partners. The team says it will have a strong commitment to open source communities such as Luther AI academic partnerships and will be pushing European values and ethical standards it says supporting fair access to modern AI research aimed at counteracting the ongoing de democratization, monopolization and loss of control or transparency. So while these are a laudable goals, and I really hope they achieve and stick to these goals. Remember that open AI has set the same at the beginning and now open AI is mostly interested in closing down access to their stuff and charging for it. But luckily, venture capitalists, which are the main founders of this venture right here, are not known to ever wanting their money back or anything like this. So this should just be a breeze for olive alpha. So I wish Jonas and co founder Samuel and anyone part of olive alpha all the best and big success in their endeavors. It's going to be fun having sort of a counterforce to the US here in Europe. Robotics 24 seven says a and pay robotics marks milestone in data pick rates for automated recycling. So speaking of companies and raising money, this company is now a raising series B for about 55 million US dollars. And they're in the space of garbage, sorting and disposal and recycling. So they've developed these analysis and gripper technologies. And this is incredibly cool to watch. I mean, we're always talking about AI taking away our jobs. I don't think people will be too sad that AI is going to take away their jobs in this particular field. So here the AI automatically analyzes the streams of garbage and sorts them by the materials in them. And so these blocks of cans just look really cool. Also, there is such a thing as waste Expo didn't know excellent must be a blast. Next news DeepMind releases a paper called open ended learning leads to generally capable agents. So what they do is they build an environment called excellent, this is kind of a 3d environment. And the agents in here you can see on the top left and top right, this is what they see, apparently, and they have to fulfill various goals in these environments, you can build any kind of environment you want in excellent, then you can tell the agents to achieve that. Apparently, the paper is about when you instruct the agents who learn multiple goals, many goals at the same time, or after one another, they become generally capable, as opposed to just having a single objective and then ending up with a very narrow skilled agent. Now excellent can be used to not only have many different environment spatially, but also have many different tasks or games in this environment. So they've captured the flag king of the\n",
            "* [ML News] BLOOM: 176B Open-Source | Chinese Brain-Scale Computer | Meta AI: No Language Left Behind: number of configurations, for example, with generative model, we've seen various benefits of having a critic a model that selects and ranks the outputs of generative models in order to make it better. And in the case with this model right here and others, we've seen numerous models where first training data is automatically generated by another model. And I think this opens up a possibility if you think of this, if you think not just what can I do with one model, how can I train one model, but think about the models that we already have and think about what you could do to use them to create training data to train other models that we usually wouldn't have enough training data for. This has been thought about obviously, for a long time, I think a lot of people when they learned about GANs for the first time, they were like, wow, we can create so much training data to train our classifiers. But this is kind of the wrong way around a generative model, like a GAN has much more information contained in it than an image classifier, which kind of reduces the space to the number of classes. So it seems like you kind of have to go from models that know less to models that know more, what exactly that entails, I think, you know, smart people will have to come up with things like this, but it's really cool to think about. And this is a really cool work. So check it out. Alright, I quickly wanted to mention this workshop here, which is held on July 28. So potentially kind of right now or something like this, depending on when this is released. This is a workshop on the leakage and reproducibility crisis in ML based science. machine learning itself obviously has a reproducibility problem. But there are also a number of machine learning based papers in other fields such as medicine, chemistry, physics, biology and whatnot. And these are apparently even worse in terms of reproducibility when they apply machine learning. So this is a workshop focusing on this various pitfalls like no train test split temporal leakage and things like pre processing on train and test sets together. Now I have to admit, I'm guilty of this. I've done this before. But if you're interested in topics like this and want to learn more, this workshop is surely a good place to go. TechCrunch writes open AI rival AI 21 labs raises $64 million to ramp up its AI powered language services yet another startup raising giant amounts of money to build giant models. I'm not exactly sure all this money flowing into this stuff is going to pay off for all of them. I mean, surely not for all of them. Is it going to pay off for a lot of them? I don't know. But I've reported on AI 21 in the past, and I think they have a\n",
            "* [ML News] OpenAI's Whisper | Meta Reads Brain Waves | AI Wins Art Fair, Annoys Humans: takes away from the artists experience. So you know, I would be happy if I were an artist or if I think of myself as an artist. But what do you think? Google releases a blog post called Ali scaling language image learning in 100 plus languages where they describe yet another large scale multimodal transformer. This time, it's a transformer that takes in text and an image and outputs text and the text that it takes in here you can see it can be some sort of a instruction to the model. So this could be a visual question answering this could be some sort of a translation. This could be the here generate all text in some language. The focus here is on multi linguality. And this is based on the pathways architecture of Google. The results are very impressive, especially considering across how many languages this model is trained and applied, and it improves performance in various metrics. Here's something for the more practical people, maybe among you more industrial people. This is a paper called operationalizing machine learning and interview study by researchers at UC Berkeley that go and interview 18 machine learning engineers about practices, tools, important learnings, and so on from machine learning in production. One interesting conclusion that I find is the one they mentioned here, ML engineering is very experimental in nature, detailing that it doesn't suddenly you know, become a straightforward thing in practice that even in operations, even in industry, where you would think, well, it's not as wild in machine learning, you're not just going to change anything all the time. Still, it is an experimental discipline. And people do need to retain at least a little bit of that research mindset, which I think is welcome and is cool and keeps things interesting. Lyon announces the release of large scale open clip. So these are larger clip models that are trained on the Lyon data sets. And these large clip models are obviously open source free to download. And they do achieve state of the art accuracies in various tasks such as zero shot image classification, and retrieval. So very cool. Check out these models. As you know, Lyon is fully kind of open source producing things in the open producing data sets, producing models and the basis for a lot of stuff that's currently happening in the community. Meta releases Blender bot three 175 billion parameter publicly available chatbot that improves its skills and safety over time. We've talked about Blender bot previously, I think I even made a video where I ran that thing locally. And I had to edit the video such that I always had to wait like two minutes to for it to respond. And I cut the video so that it looked like it was video so that it looked like so that people aren't bored essentially looked like it responded immediately. I will not be able to run this thing but\n",
            "\n",
            " Q: Is OpenAI 'open'?\n",
            " A:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = \"For more information, check out my following videos: \"\n",
        "for source in sources[:2]:\n",
        "    src_lst = eval(source)\n",
        "    title = \"\".join(src_lst[1])\n",
        "    url = \"\".join(src_lst[2])\n",
        "    final_src = title + \" \" + url\n",
        "    res += \" \" + final_src\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w835M5jchC85",
        "outputId": "72e36a44-5b5b-460e-b91b-03c016c66c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For more information, check out my following videos:  [ML News] AI-generated patent approved | Germany gets an analog to OpenAI | ML cheats video games https://www.youtube.com/watch?v=SPOqoI0zOPQ [ML News] BLOOM: 176B Open-Source | Chinese Brain-Scale Computer | Meta AI: No Language Left Behind https://www.youtube.com/watch?v=W3mrgqtm5R4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_query_with_context(\"Who is Yann LeCun\", df, document_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uKg6tvEaHAI",
        "outputId": "2ee98f6c-1af2-4084-b45e-d870e5cd341b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 4 document sections:\n",
            "(2944, '[Drama] Yann LeCun against Twitter on Dataset Bias', 'https://www.youtube.com/watch?v=n1SXlK5rhR8')\n",
            "(2943, '[Drama] Yann LeCun against Twitter on Dataset Bias', 'https://www.youtube.com/watch?v=n1SXlK5rhR8')\n",
            "(741, \"[ML News] DeepMind controls fusion | Yann LeCun's JEPA architecture | US: AI can't copyright its art\", 'https://www.youtube.com/watch?v=YOLL8dIhLJI')\n",
            "(1958, 'Yann LeCun - Self-Supervised Learning: The Dark Matter of Intelligence (FAIR Blog Post Explained)', 'https://www.youtube.com/watch?v=Ag1bw8MfHGQ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FnS7w2Zna8pG",
        "outputId": "e0014566-9531-44b2-db07-49edc1969450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yann LeCun is a French computer scientist and AI researcher. He is the Silver Professor of the Courant Institute of Mathematical Sciences, a professor of computer science at the Center for Data Science, and a professor of neural science at the New York University. He is also the founding director of the NYU Center for Data Science and the director of AI Research at Facebook. For more information, check out my following videos:  [Drama] Yann LeCun against Twitter on Dataset Bias https://www.youtube.com/watch?v=n1SXlK5rhR8 [Drama] Yann LeCun against Twitter on Dataset Bias https://www.youtube.com/watch?v=n1SXlK5rhR8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiUvv7HUfP2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}